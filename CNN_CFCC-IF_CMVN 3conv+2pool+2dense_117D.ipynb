{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in and out data\n",
    "dimension = 36\n",
    "context_frames = 100\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,dimension,context_frames]) #inputs\n",
    "y = tf.placeholder(tf.float32,[None,2]) #labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 36, 100, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 34, 98, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#conv layer 1\n",
    "#[filter_height, filter_width, in-channels, out-channels]\n",
    "weights_conv1 = tf.Variable(tf.truncated_normal(shape = [3,3,1,128], stddev = 0.01, dtype = tf.float32))\n",
    "bias_conv1 = tf.Variable(tf.constant(0.1,shape = [128]))\n",
    "\n",
    "# reshape of input -1 dynamically calculates the batch_size,in_height,in_width,in_channels\n",
    "input_x = tf.reshape(x,[-1,dimension,context_frames,1])\n",
    "print(input_x)\n",
    "conv1 = tf.nn.conv2d(input_x,weights_conv1, strides = [1,1,1,1],padding = 'VALID')\n",
    "conv1_out = tf.nn.relu(conv1+bias_conv1)\n",
    "print(conv1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_1:0\", shape=(?, 32, 96, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#conv layer 2\n",
    "weights_conv2 = tf.Variable(tf.truncated_normal(shape = [3,3,128,64], stddev = 0.01, dtype = tf.float32))\n",
    "bias_conv2 = tf.Variable(tf.constant(0.1,shape = [64]))\n",
    "\n",
    "conv2 = tf.nn.conv2d(conv1_out,weights_conv2,strides = [1,1,1,1], padding = 'VALID')\n",
    "conv2_out = tf.nn.relu(conv2+bias_conv2)\n",
    "print(conv2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_2:0\", shape=(?, 14, 46, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#conv layer 3\n",
    "weights_conv3 = tf.Variable(tf.truncated_normal(shape = [5,5,64,64], stddev = 0.01, dtype = tf.float32))\n",
    "bias_conv3 = tf.Variable(tf.constant(0.1,shape = [64]))\n",
    "\n",
    "conv3 = tf.nn.conv2d(conv2_out,weights_conv3,strides = [1,2,2,1], padding = 'VALID')\n",
    "conv3_out = tf.nn.relu(conv3+bias_conv3)\n",
    "print(conv3_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max_pooling2d/MaxPool:0\", shape=(?, 9, 41, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#max pool 1\n",
    "pool1_out = tf.layers.max_pooling2d(conv3_out, pool_size = [6,6], strides = [1,1], padding = 'VALID')\n",
    "#conv4_out = tf.nn.relu(conv4+bias_conv4)\n",
    "print(pool1_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"max_pooling2d_2/MaxPool:0\", shape=(?, 4, 36, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#max pool 2\n",
    "pool2_out = tf.layers.max_pooling2d(pool1_out, pool_size = [6,6], strides = [1,1], padding = 'VALID')\n",
    "#conv4_out = tf.nn.relu(conv4+bias_conv4)\n",
    "print(pool2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.tensor_shape.Dimension'> 9216\n",
      "Tensor(\"Relu_3:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#print(pool_out.shape[1])\n",
    "reduced_dim = pool2_out.shape[1] * pool2_out.shape[2] * pool2_out.shape[3]\n",
    "print(type(reduced_dim),int(reduced_dim))\n",
    "\n",
    "#dense layer 1\n",
    "weights_dense1 = tf.Variable(tf.truncated_normal([int(reduced_dim),256], stddev = 0.01, dtype = tf.float32))\n",
    "bias_dense1 = tf.Variable(tf.constant(0.1,shape = [256]))\n",
    "\n",
    "input_dense = tf.reshape(pool2_out, shape = [-1,int(reduced_dim)])\n",
    "#print(input_dense)\n",
    "dense1_sum = tf.matmul(input_dense,weights_dense1)+bias_dense1\n",
    "dense1_output = tf.nn.relu(dense1_sum)\n",
    "print(dense1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout/mul:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#adding dropout to dense layer 1\n",
    "keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "dense1_dropout = tf.nn.dropout(dense1_output,keep_prob)\n",
    "print(dense1_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_4:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# dense layer 2\n",
    "weights_dense2 = tf.Variable(tf.truncated_normal([256,256], stddev = 0.01, dtype = tf.float32))\n",
    "bias_dense2 = tf.Variable(tf.constant(0.1,shape = [256]))\n",
    "\n",
    "dense2_sum = tf.matmul(dense1_dropout,weights_dense2)+ bias_dense2\n",
    "dense2_output = tf.nn.relu(dense2_sum)\n",
    "print(dense2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_1/mul:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dense2_dropout = tf.nn.dropout(dense2_output,keep_prob)\n",
    "print(dense2_dropout)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dense layer 3\n",
    "weights_dense3 = tf.Variable(tf.truncated_normal([512,512], stddev = 0.01, dtype = tf.float32))\n",
    "bias_dense3 = tf.Variable(tf.constant(0.1,shape = [512]))\n",
    "\n",
    "dense3_sum = tf.matmul(dense2_dropout,weights_dense3)+ bias_dense3\n",
    "dense3_output = tf.nn.relu(dense3_sum)\n",
    "print(dense3_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dense3_dropout = tf.nn.dropout(dense3_output,keep_prob)\n",
    "print(dense3_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#output layer\n",
    "output_weights = tf.Variable(tf.truncated_normal(shape = [256,2], stddev = 0.01, dtype = tf.float32))\n",
    "output_bias = tf.Variable(tf.constant(0.1,shape = [2]))\n",
    "\n",
    "output_sum = tf.matmul(dense2_dropout,output_weights)+output_bias\n",
    "final_output = tf.nn.softmax(output_sum)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses and optimizers\n",
    "\n",
    "cross_entropy = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = final_output))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "correct_predictions = tf.equal( tf.argmax(final_output, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean( tf.cast( correct_predictions, tf.float32 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing tensors using session\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#saver = tf.train.Saver(max_to_keep = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eer(scores_true, scores_false):\n",
    "    \n",
    "    \"\"\" Calculate EER\n",
    "        Args:\n",
    "            scores_true: A score list of positive samples\n",
    "            scores_false: A score list of negative samples\n",
    "        Return:\n",
    "            (EER, threshold)\n",
    "        Note:\n",
    "            Here, the threshold is set, and if the score is higher than or equal to the threshold,\n",
    "            the decision is passed, and the decision is rejected as below.\n",
    "    \"\"\"\n",
    "    min1 = min(scores_true)\n",
    "    min2 = min(scores_false)\n",
    "    low = min(min1, min2)\n",
    "    max1 = max(scores_true)\n",
    "    max2 = max(scores_false)\n",
    "    high = max(max1, max2)\n",
    "    FAR = -55\n",
    "    FRR = -55\n",
    "    mid = -100\n",
    "    # Bisection find threshold\n",
    "    while True:\n",
    "        bef_FAR = FAR \n",
    "        bef_FRR = FRR\n",
    "        bef_mid = mid\n",
    "        mid = (high + low) / 2\n",
    "        FRR = sum([1 for s in scores_true if s < mid]) / len(scores_true)\n",
    "        FAR = sum([1 for s in scores_false if s >= mid]) / len(scores_false)\n",
    "        #print(\"FRR  \",FRR,\" FAR   \",FAR,\"   mid   \",mid)\n",
    "\n",
    "        if abs(FRR - FAR) < 1e-7:\n",
    "            print(\"FRR  \",FRR,\" FAR   \",FAR,\"   mid   \",mid)\n",
    "            break\n",
    "\n",
    "        if FRR < FAR:\n",
    "            low = mid\n",
    "        else:\n",
    "            high = mid\n",
    "            \n",
    "        if bef_FAR == FAR :\n",
    "            if bef_FRR == FRR :\n",
    "                if bef_mid == mid :\n",
    "                    print(\"FRR  \",FRR,\" FAR   \",FAR,\"   mid   \",mid)\n",
    "                    break\n",
    "\n",
    "    EER  =  FRR\n",
    "    return EER, (high + low) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "\n",
    "def get_EER(test_path, index, sp_path ):\n",
    "    scores = []\n",
    "    development_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    #test_path = \"C:\\\\Users\\\\project1\\\\Desktop\\\\Siva\\\\HT_IFCC_hemlata\\\\CNN_input_testing_dev\"\n",
    "    dev_num = len(list(os.listdir(test_path)))//2\n",
    "    \n",
    "    for j in range(1, dev_num+1):\n",
    "        filename = \"features_\"+str(j)+\".npy\"\n",
    "        labelsname= \"labels_\"+str(j)+\".npy\"\n",
    "        \n",
    "        features = np.load(os.path.join(test_path, filename))\n",
    "        labels = np.load(os.path.join(test_path, labelsname))\n",
    "        patches = labels.shape[0]\n",
    "        patch_prob = session.run(final_output, feed_dict = {x:features, keep_prob : 1.0})\n",
    "        final_prob = np.zeros([2])\n",
    "        \n",
    "        for i in range(patches):\n",
    "            if(patch_prob[i][0] == 0):\n",
    "                patch_prob[i][0] = 1e-25\n",
    "            if(patch_prob[i][1] == 0):\n",
    "                patch_prob[i][1] = 1e-25\n",
    "                \n",
    "            final_prob[0] = final_prob[0] + np.log(patch_prob[i][0])\n",
    "            final_prob[1] = final_prob[1] + np.log(patch_prob[i][1])\n",
    "\n",
    "        final_prob = final_prob / patches\n",
    "        score = (final_prob[0]) - (final_prob[1])\n",
    "        audio_label = labels[0]\n",
    "\n",
    "        scores.append(score)\n",
    "        probabilities.append( np.mean(patch_prob, axis=0) )\n",
    "        development_labels.append( audio_label )\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    development_labels = np.array(development_labels)\n",
    "    probabilities = np.array(probabilities)\n",
    "\n",
    "    scores_geniue = []\n",
    "    scores_spoof = []\n",
    "    prob_genuine = []\n",
    "    prob_spoof = []\n",
    "\n",
    "    for i in range(development_labels.shape[0]):\n",
    "        if development_labels[i][0] == 1 :\n",
    "            scores_geniue.append(scores[i])\n",
    "            prob_genuine.append(probabilities[i])\n",
    "        elif development_labels[i][1] == 1:\n",
    "            scores_spoof.append(scores[i])\n",
    "            prob_spoof.append(probabilities[i])\n",
    "\n",
    "    scores_geniue = np.array(scores_geniue)\n",
    "    scores_spoof = np.array(scores_spoof)\n",
    "    prob_genuine = np.array(prob_genuine)\n",
    "    prob_spoof = np.array(prob_spoof)\n",
    "    \n",
    "    a,b = cal_eer(scores_geniue, scores_spoof)\n",
    "    \n",
    "    score_path = os.path.join(sp_path, \"scores\")\n",
    "    prob_path = os.path.join(sp_path, \"probabilities\")\n",
    "    \n",
    "    if not os.path.exists(score_path):\n",
    "        os.makedirs(score_path)\n",
    "    if not os.path.exists(prob_path):\n",
    "        os.makedirs(prob_path)\n",
    "        \n",
    "    prob_file = \"probabilities_\"+str(index)\n",
    "    sc_file = \"scores_\"+str(index)\n",
    "    savemat(os.path.join(score_path, sc_file), { 'genuine':scores_geniue, 'spoof':scores_spoof, 'eer' : a*100 })\n",
    "    savemat(os.path.join(prob_path, prob_file), { 'genuine':prob_genuine, 'spoof':prob_spoof })\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Aditya\\AnkurSir\\CFCC_IF\\36D\\overlap_36D\n"
     ]
    }
   ],
   "source": [
    "#Training and testing accuracy\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "session.run(init)\n",
    "\n",
    "features_path = \"E:\\\\Aditya\\\\AnkurSir\\\\CFCC_IF\\\\36D\\\\overlap_36D\"\n",
    "print(features_path)\n",
    "development_path = \"E:\\\\Aditya\\\\AnkurSir\\\\CFCC_IF\\\\36D\\\\devoverlapData_36D\"\n",
    "# evaluation_path = \"E:\\\\Aditya\\\\ASVspoof2017_CFCC_IF_CMVN\\\\evaloverlapData_117D\"\n",
    "model_path = \"E:\\\\Aditya\\\\AnkurSir\\\\CFCC_IF\\\\36D\\\\Models\\\\\"\n",
    "    \n",
    "dev_sp_path = \"E:\\\\Aditya\\\\AnkurSir\\\\CFCC_IF\\\\36D\\\\dev\\\\\"\n",
    "eva_sp_path = \"E:\\\\Aditya\\\\ASVspoof2017_CFCC_IF_CMVN\\\\eval_sp_117D_CNN_3c2p2d\" \n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if not os.path.exists(dev_sp_path):\n",
    "    os.makedirs(dev_sp_path)\n",
    "    \n",
    "batches = (len(list(os.listdir(features_path))))//2\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b43aa5e8a451>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#print(\"before  \",str(j),\"   \",accuracy.eval(feed_dict = {x:features, y:labels, keep_prob:1.0}))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m#print(\"after   \",str(j),\"   \",accuracy.eval(feed_dict = {x:features, y:labels, keep_prob:1.0}))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1742\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1743\u001b[0m     \"\"\"\n\u001b[1;32m-> 1744\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4118\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4119\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4120\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    \n",
    "    for j in range(1,batches+1):  #number of batches\n",
    "\n",
    "\n",
    "        filename = \"batch_\"+str(j)+\".npy\"\n",
    "        labelsname= \"label_\"+str(j)+\".npy\"\n",
    "\n",
    "        features = np.load(os.path.join(features_path, filename))\n",
    "        labels = np.load(os.path.join(features_path, labelsname))\n",
    "        \n",
    "        print(np.sum(np.isnan(features)))\n",
    "        print(np.sum(np.isnan(labels)))\n",
    "        \n",
    "        #print(\"before  \",str(j),\"   \",accuracy.eval(feed_dict = {x:features, y:labels, keep_prob:1.0}))\n",
    "        train_step.run(session = session,feed_dict={ x:features, y:labels, keep_prob:1.0})\n",
    "        #print(\"after   \",str(j),\"   \",accuracy.eval(feed_dict = {x:features, y:labels, keep_prob:1.0}))\n",
    "\n",
    "\n",
    "    save_path = saver.save(session,os.path.join(model_path,\"Model_\"+str(i+1)))\n",
    "\n",
    "    print(\"iteration  \",(i+1))\n",
    "    eer_dev = get_EER(development_path, (i+1), dev_sp_path)\n",
    "    print(\" develop eer  \",eer_dev)\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "with tf.Session() as session:\n",
    "    saver.restore(session,os.path.join(model_path,\"Model_\"+str(50)))\n",
    "    \n",
    "    for i in range(50,75):\n",
    "        for j in range(1,batches+1):\n",
    "            \n",
    "            filename = \"batch_\"+str(j)+\".npy\"\n",
    "            labelsname= \"label_\"+str(j)+\".npy\"\n",
    "\n",
    "            features = np.load(os.path.join(features_path, filename))\n",
    "            labels = np.load(os.path.join(features_path, labelsname))\n",
    "            \n",
    "            train_step.run(session = session,feed_dict={ x:features, y:labels, keep_prob:0.5})\n",
    "        save_path = saver.save(session,os.path.join(model_path,\"Model_\"+str(i+1)))\n",
    "        \n",
    "        print(\" iteration   \", i+1)\n",
    "        eer_dev = get_EER(development_path, (i+1), dev_sp_path)\n",
    "        print(\" develop eer\"    , eer_dev)\n",
    "        print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation_path = \"E:\\\\Aditya\\\\ASVspoof2017_CFCC_IF_CMVN\\\\evaloverlapData_117D\"\n",
    "eva_sp_path = \"E:\\\\Aditya\\\\ASVspoof2017_CFCC_IF_CMVN\\\\eval_sp_117D_CNN_3c2p2d\" \n",
    "\n",
    "if not os.path.exists(eva_sp_path):\n",
    "    os.makedirs(eva_sp_path)\n",
    "    \n",
    "with tf.Session() as session:\n",
    "    for i in range(1,50):\n",
    "        saver.restore(session,os.path.join(model_path,\"Model_\"+str(i+1)))\n",
    "        print(\"iteration \", (i+1))\n",
    "        eval_err = get_EER(evaluation_path, (i+1), eva_sp_path)\n",
    "        print(\" evaluation err  \", eval_err)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(5):\n",
    "    for j in range(1,batches+1):  #number of batches\n",
    "        filename = \"batch_\"+str(j)+\".npy\"\n",
    "        labelsname= \"label_\"+str(j)+\".npy\"\n",
    "        \n",
    "        features = np.load(os.path.join(features_path, filename))\n",
    "        labels = np.load(os.path.join(features_path, labelsname))\n",
    "       \n",
    "        #print(\"before  \",str(j),\"   \",accuracy.eval(feed_dict = {x:features, y:labels, keep_prob:1.0}))\n",
    "        train_step.run(session = session,feed_dict={ x:features, y:labels, keep_prob:0.5})\n",
    "        #print(\"after   \",str(j),\"   \",accuracy.eval(feed_dict = {x:features, y:labels, keep_prob:1.0}))\n",
    "        \n",
    "   \n",
    "    save_path = saver.save(session,os.path.join(model_path,\"Model\"+str(i+1)))\n",
    "    \n",
    "    print(\"iteration  \",(i+1))\n",
    "    eer_dev = get_EER(development_path, (i+1), dev_sp_path)\n",
    "    print(\" develop eer  \",eer_dev)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "development_path = \"C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC_CMVN\\\\devoverlapData\"\n",
    "dev_num = os.listdir(development_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC_CMVN\\\\overlapData_90D\"\n",
    "batches = (len(list(os.listdir(features_path))))//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
