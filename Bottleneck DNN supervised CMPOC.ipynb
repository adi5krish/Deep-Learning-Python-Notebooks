{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_size = 60\n",
    "context = 5\n",
    "\n",
    "input_x = tf.placeholder(dtype = tf.float32, shape = [None, feat_size*(2*context +1)])\n",
    "output_y = tf.placeholder(dtype = tf.float32, shape = [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#hidden layer 1\n",
    "\n",
    "hidden_1 = 512\n",
    "hidden_1_weights = tf.get_variable( \"hidden_1_weights\",\n",
    "                                  shape = [ feat_size*(2*context +1), hidden_1],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_1_bias = tf.get_variable( \"hidden_1_bias\",\n",
    "                                    shape = [hidden_1],\n",
    "                                    dtype = tf.float32,\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_1_dnn = tf.matmul( input_x, hidden_1_weights) + hidden_1_bias\n",
    "hidden_1_out = tf.nn.relu(hidden_1_dnn)\n",
    "print(hidden_1_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_1:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#hidden layer 2\n",
    "\n",
    "hidden_2 = 512\n",
    "hidden_2_weights = tf.get_variable( \"hidden_2_weights\",\n",
    "                                  shape = [ hidden_1, hidden_2],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_2_bias = tf.get_variable( \"hidden_2_bias\",\n",
    "                                    shape = [hidden_2],\n",
    "                                    dtype = tf.float32,\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_2_dnn = tf.matmul( hidden_1_out, hidden_2_weights) + hidden_2_bias\n",
    "hidden_2_out = tf.nn.relu(hidden_2_dnn)\n",
    "print(hidden_2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_2:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#hidden layer 2\n",
    "\n",
    "hidden_ = 512\n",
    "hidden_weights = tf.get_variable( \"hidden_weights\",\n",
    "                                  shape = [ hidden_2, hidden_],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_bias = tf.get_variable( \"hidden_bias\",\n",
    "                                    shape = [hidden_],\n",
    "                                    dtype = tf.float32,\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_dnn = tf.matmul( hidden_2_out, hidden_weights) + hidden_bias\n",
    "hidden_out = tf.nn.relu(hidden_dnn)\n",
    "print(hidden_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_3:0\", shape=(?, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#bottleneck layer \n",
    "\n",
    "bottleneck_dim = 40\n",
    "bottleneck_weights = tf.get_variable( \"bottleneck_weights\",\n",
    "                                  shape = [ hidden_, bottleneck_dim],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bottleneck_bias = tf.get_variable( \"bottleneck_bias\",\n",
    "                                    shape = [bottleneck_dim],\n",
    "                                    dtype = tf.float32,\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bottleneck_dnn = tf.matmul( hidden_out, bottleneck_weights) + bottleneck_bias\n",
    "bottleneck_out = tf.nn.relu(bottleneck_dnn)\n",
    "print(bottleneck_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_4:0\", shape=(?, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#hidden layer 4\n",
    "\n",
    "hidden_4 = 200\n",
    "hidden_4_weights = tf.get_variable( \"hidden_4_weights\",\n",
    "                                  shape = [ bottleneck_dim, hidden_4],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_4_bias = tf.get_variable( \"hidden_4_bias\",\n",
    "                                    shape = [hidden_4],\n",
    "                                    dtype = tf.float32,\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_4_dnn = tf.matmul( bottleneck_out, hidden_4_weights) + hidden_4_bias\n",
    "hidden_4_out = tf.nn.relu(hidden_4_dnn)\n",
    "print(hidden_4_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_5:0\", shape=(?, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#hidden layer 1\n",
    "\n",
    "hidden_5 = 200\n",
    "hidden_5_weights = tf.get_variable( \"hidden_5_weights\",\n",
    "                                  shape = [ hidden_4, hidden_5],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_5_bias = tf.get_variable( \"hidden_5_bias\",\n",
    "                                    shape = [hidden_5],\n",
    "                                    dtype = tf.float32,\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden_5_dnn = tf.matmul( hidden_4_out, hidden_5_weights) + hidden_5_bias\n",
    "hidden_5_out = tf.nn.relu(hidden_5_dnn)\n",
    "print(hidden_5_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#output layer\n",
    "\n",
    "final = 2\n",
    "final_weights = tf.get_variable( \"final_weights\",\n",
    "                                  shape = [ hidden_5, final],\n",
    "                                  dtype = tf.float32,\n",
    "                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "final_bias = tf.get_variable( \"final_bias\",\n",
    "                                    shape = [final],\n",
    "                                    dtype = tf.float32,\n",
    "                                   initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "final_dnn = tf.matmul( hidden_5_out, final_weights) + final_bias\n",
    "final_out = tf.nn.softmax(final_dnn)\n",
    "print(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses and optimizers\n",
    "\n",
    "cross_entropy = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels = output_y, logits = final_dnn))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "correct_predictions = tf.equal( tf.argmax(final_dnn, 1), tf.argmax(output_y, 1))\n",
    "accuracy = tf.reduce_mean( tf.cast( correct_predictions, tf.float32 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing tensors using sessoion\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "features_path = 'C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC\\\\contextData'\n",
    "development_path = 'C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC\\\\devcontextData'\n",
    "\n",
    "model_path = 'C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC\\\\Models'\n",
    "\n",
    "dev_sp_path = 'C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC\\\\Dev_sp'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_eer(scores_true, scores_false):\n",
    "    \n",
    "    \"\"\" Calculate EER\n",
    "        Args:\n",
    "            scores_true: A score list of positive samples\n",
    "            scores_false: A score list of negative samples\n",
    "        Return:\n",
    "            (EER, threshold)\n",
    "        Note:\n",
    "            Here, the threshold is set, and if the score is higher than or equal to the threshold,\n",
    "            the decision is passed, and the decision is rejected as below.\n",
    "    \"\"\"\n",
    "    min1 = min(scores_true)\n",
    "    min2 = min(scores_false)\n",
    "    low = min(min1, min2)\n",
    "    max1 = max(scores_true)\n",
    "    max2 = max(scores_false)\n",
    "    high = max(max1, max2)\n",
    "    FAR = -55\n",
    "    FRR = -55\n",
    "    mid = -100\n",
    "    # Bisection find threshold\n",
    "    while True:\n",
    "        bef_FAR = FAR \n",
    "        bef_FRR = FRR\n",
    "        bef_mid = mid\n",
    "        mid = (high + low) / 2\n",
    "        FRR = sum([1 for s in scores_true if s < mid]) / len(scores_true)\n",
    "        FAR = sum([1 for s in scores_false if s >= mid]) / len(scores_false)\n",
    "        #print(\"FRR  \",FRR,\" FAR   \",FAR,\"   mid   \",mid)\n",
    "\n",
    "        if abs(FRR - FAR) < 1e-7:\n",
    "            print(\"FRR  \",FRR,\" FAR   \",FAR,\"   mid   \",mid)\n",
    "            break\n",
    "\n",
    "        if FRR < FAR:\n",
    "            low = mid\n",
    "        else:\n",
    "            high = mid\n",
    "            \n",
    "        if bef_FAR == FAR :\n",
    "            if bef_FRR == FRR :\n",
    "                if bef_mid == mid :\n",
    "                    print(\"FRR  \",FRR,\" FAR   \",FAR,\"   mid   \",mid)\n",
    "                    break\n",
    "\n",
    "    EER  =  FRR\n",
    "    return EER, (high + low) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "\n",
    "def get_EER(test_path, index, sp_path ):\n",
    "    scores = []\n",
    "    development_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    #test_path = \"C:\\\\Users\\\\project1\\\\Desktop\\\\Siva\\\\HT_IFCC_hemlata\\\\CNN_input_testing_dev\"\n",
    "    indexs = (len(list(os.listdir(test_path))))//2\n",
    "    \n",
    "    for folders in range(1, indexs+1):\n",
    "        features = np.load(os.path.join(test_path, \"features\"+str(folders)+\".npy\"))\n",
    "        labels = np.load(os.path.join(test_path, \"labels\"+str(folders)+\".npy\"))\n",
    "        patches = labels.shape[0]\n",
    "        patch_prob = session.run(final_out, feed_dict = {input_x:features})\n",
    "        final_prob = np.zeros([2])\n",
    "        \n",
    "        for i in range(patches):\n",
    "            if(patch_prob[i][0] == 0):\n",
    "                patch_prob[i][0] = 1e-25\n",
    "            if(patch_prob[i][1] == 0):\n",
    "                patch_prob[i][1] = 1e-25\n",
    "                \n",
    "            final_prob[0] = final_prob[0] + np.log(patch_prob[i][0])\n",
    "            final_prob[1] = final_prob[1] + np.log(patch_prob[i][1])\n",
    "\n",
    "        final_prob = final_prob / patches\n",
    "        score = (final_prob[0]) - (final_prob[1])\n",
    "        audio_label = labels[0]\n",
    "\n",
    "        scores.append(score)\n",
    "        probabilities.append( np.mean(patch_prob, axis=0) )\n",
    "        development_labels.append( audio_label )\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    development_labels = np.array(development_labels)\n",
    "    probabilities = np.array(probabilities)\n",
    "\n",
    "    scores_geniue = []\n",
    "    scores_spoof = []\n",
    "    prob_genuine = []\n",
    "    prob_spoof = []\n",
    "\n",
    "    for i in range(development_labels.shape[0]):\n",
    "        if development_labels[i][0] == 1 :\n",
    "            scores_geniue.append(scores[i])\n",
    "            prob_genuine.append(probabilities[i])\n",
    "        elif development_labels[i][1] == 1:\n",
    "            scores_spoof.append(scores[i])\n",
    "            prob_spoof.append(probabilities[i])\n",
    "\n",
    "    scores_geniue = np.array(scores_geniue)\n",
    "    scores_spoof = np.array(scores_spoof)\n",
    "    prob_genuine = np.array(prob_genuine)\n",
    "    prob_spoof = np.array(prob_spoof)\n",
    "    \n",
    "    a,b = cal_eer(scores_geniue, scores_spoof)\n",
    "    \n",
    "    score_path = os.path.join(sp_path, \"scores\")\n",
    "    prob_path = os.path.join(sp_path, \"probabilities\")\n",
    "    \n",
    "    if not os.path.exists(score_path):\n",
    "        os.makedirs(score_path)\n",
    "        \n",
    "    if not os.path.exists(prob_path):\n",
    "        os.makedirs(prob_path)\n",
    "        \n",
    "    prob_file = \"probabilities_\"+str(index)\n",
    "    sc_file = \"scores_\"+str(index)\n",
    "    savemat(os.path.join(score_path, sc_file), { 'genuine':scores_geniue, 'spoof':scores_spoof, 'eer' : a*100 })\n",
    "    savemat(os.path.join(prob_path, prob_file), { 'genuine':prob_genuine, 'spoof':prob_spoof })\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model0 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   1\n",
      "FRR   0.1118421052631579  FAR    0.11157894736842106    mid    5.82786312413\n",
      " develop eer   0.1118421052631579\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model1 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   2\n",
      "FRR   0.0868421052631579  FAR    0.08736842105263158    mid    4.95555855585\n",
      " develop eer   0.0868421052631579\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model2 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   3\n",
      "FRR   0.08421052631578947  FAR    0.08421052631578947    mid    7.45598241372\n",
      " develop eer   0.08421052631578947\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model3 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   4\n",
      "FRR   0.11578947368421053  FAR    0.11578947368421053    mid    5.1582229602\n",
      " develop eer   0.11578947368421053\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model4 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   5\n",
      "FRR   0.13026315789473683  FAR    0.12947368421052632    mid    5.02738701684\n",
      " develop eer   0.13026315789473683\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model5 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   6\n",
      "FRR   0.14078947368421052  FAR    0.14105263157894737    mid    4.77285696799\n",
      " develop eer   0.14078947368421052\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model6 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   7\n",
      "FRR   0.1394736842105263  FAR    0.14    mid    5.06823367876\n",
      " develop eer   0.1394736842105263\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model7 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   8\n",
      "FRR   0.13289473684210526  FAR    0.13263157894736843    mid    5.20630493192\n",
      " develop eer   0.13289473684210526\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model8 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   9\n",
      "FRR   0.125  FAR    0.12421052631578948    mid    6.293149772\n",
      " develop eer   0.125\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model9 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   10\n",
      "FRR   0.11710526315789474  FAR    0.11789473684210526    mid    7.33217488103\n",
      " develop eer   0.11710526315789474\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model10 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   11\n",
      "FRR   0.125  FAR    0.12526315789473685    mid    6.73176204628\n",
      " develop eer   0.125\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model11 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   12\n",
      "FRR   0.12105263157894737  FAR    0.12210526315789473    mid    7.23946550262\n",
      " develop eer   0.12105263157894737\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model12 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   13\n",
      "FRR   0.11973684210526316  FAR    0.12    mid    7.42390879336\n",
      " develop eer   0.11973684210526316\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model13 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   14\n",
      "FRR   0.10657894736842105  FAR    0.10736842105263159    mid    6.97489765706\n",
      " develop eer   0.10657894736842105\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model14 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   15\n",
      "FRR   0.11315789473684211  FAR    0.11263157894736842    mid    7.76905153304\n",
      " develop eer   0.11315789473684211\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model15 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   16\n",
      "FRR   0.11315789473684211  FAR    0.11368421052631579    mid    7.8461045331\n",
      " develop eer   0.11315789473684211\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model16 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   17\n",
      "FRR   0.1118421052631579  FAR    0.11263157894736842    mid    9.27189773582\n",
      " develop eer   0.1118421052631579\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model17 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   18\n",
      "FRR   0.10657894736842105  FAR    0.10736842105263159    mid    6.67906359248\n",
      " develop eer   0.10657894736842105\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model18 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   19\n",
      "FRR   0.09736842105263158  FAR    0.0968421052631579    mid    8.69943039205\n",
      " develop eer   0.09736842105263158\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model19 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   20\n",
      "FRR   0.09868421052631579  FAR    0.09789473684210526    mid    9.94191808807\n",
      " develop eer   0.09868421052631579\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model20 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   21\n",
      "FRR   0.1013157894736842  FAR    0.10105263157894737    mid    11.5552683825\n",
      " develop eer   0.1013157894736842\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model21 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   22\n",
      "FRR   0.10394736842105264  FAR    0.1031578947368421    mid    11.9919248206\n",
      " develop eer   0.10394736842105264\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model22 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   23\n",
      "FRR   0.11052631578947368  FAR    0.11157894736842106    mid    11.4861936058\n",
      " develop eer   0.11052631578947368\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model23 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   24\n",
      "FRR   0.09736842105263158  FAR    0.0968421052631579    mid    10.2903349247\n",
      " develop eer   0.09736842105263158\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model24 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   25\n",
      "FRR   0.09736842105263158  FAR    0.09789473684210526    mid    10.3862970661\n",
      " develop eer   0.09736842105263158\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model25 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   26\n",
      "FRR   0.12236842105263158  FAR    0.12210526315789473    mid    12.7759842864\n",
      " develop eer   0.12236842105263158\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model26 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   27\n",
      "FRR   0.10526315789473684  FAR    0.10631578947368421    mid    12.5827650132\n",
      " develop eer   0.10526315789473684\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model27 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   28\n",
      "FRR   0.09605263157894736  FAR    0.0968421052631579    mid    10.3567866126\n",
      " develop eer   0.09605263157894736\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model28 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   29\n",
      "FRR   0.10263157894736842  FAR    0.10210526315789474    mid    12.179825882\n",
      " develop eer   0.10263157894736842\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model29 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   30\n",
      "FRR   0.09736842105263158  FAR    0.0968421052631579    mid    10.7574048723\n",
      " develop eer   0.09736842105263158\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model30 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   31\n",
      "FRR   0.10921052631578948  FAR    0.10842105263157895    mid    11.066207425\n",
      " develop eer   0.10921052631578948\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model31 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   32\n",
      "FRR   0.10526315789473684  FAR    0.10526315789473684    mid    11.9685937324\n",
      " develop eer   0.10526315789473684\n",
      "\n",
      "INFO:tensorflow:C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model32 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "iteration   33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-725177bdcb4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iteration  \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0meer_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_EER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevelopment_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_sp_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" develop eer  \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meer_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-4b11c7333f5d>\u001b[0m in \u001b[0;36mget_EER\u001b[1;34m(test_path, index, sp_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfolders\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mpatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0m_ZIP_PREFIX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb'PK\\x03\\x04'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0mmagic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[1;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# to seek past the beginning of the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training and testing accuracy\n",
    "session.run(init)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if not os.path.exists(dev_sp_path):\n",
    "    os.makedirs(dev_sp_path)\n",
    "    \n",
    "batches = (len(list(os.listdir(features_path))))//2\n",
    "    \n",
    "for i in range(0,100):\n",
    "    for j in range(1,batches+1):  #number of batches\n",
    "        filename = \"batch_\"+str(j)+\".npy\"\n",
    "        labelsname= \"label_\"+str(j)+\".npy\"\n",
    "        \n",
    "        features = np.load(os.path.join(features_path, filename))\n",
    "        labels = np.load(os.path.join(features_path, labelsname))\n",
    "\n",
    "        train_step.run(session = session,feed_dict={ input_x:features, output_y:labels })\n",
    "        \n",
    "    save_path = saver.save(session, os.path.join(model_path,\"Model\"+str(i)))\n",
    "    \n",
    "    print(\"iteration  \",(i+1))\n",
    "    eer_dev = get_EER(development_path, (i+1), dev_sp_path)\n",
    "    print(\" develop eer  \",eer_dev)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model0\n",
      "iteration   1\n",
      "FRR   0.32280431432973805  FAR    0.3227848101265823    mid    7.66412907359\n",
      " evaluat eer   0.32280431432973805\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model1\n",
      "iteration   2\n",
      "FRR   0.3489984591679507  FAR    0.3489340439706862    mid    7.30289691482\n",
      " evaluat eer   0.3489984591679507\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model2\n",
      "iteration   3\n",
      "FRR   0.3443759630200308  FAR    0.3448534310459693    mid    11.4423173757\n",
      " evaluat eer   0.3443759630200308\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model3\n",
      "iteration   4\n",
      "FRR   0.37057010785824346  FAR    0.37108594270486345    mid    6.85604165258\n",
      " evaluat eer   0.37057010785824346\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HIMANSHU\\Desktop\\Aditya\\ASVspoof2017_CMPOC\\Models\\Model4\n",
      "iteration   5\n",
      "FRR   0.37057010785824346  FAR    0.3705862758161226    mid    6.88163990388\n",
      " evaluat eer   0.37057010785824346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_path = 'C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC\\\\evalcontextData'\n",
    "eval_sp_path = 'C:\\\\Users\\\\HIMANSHU\\\\Desktop\\\\Aditya\\\\ASVspoof2017_CMPOC\\\\Eval_sp'\n",
    "\n",
    "with tf.Session() as session:\n",
    "    for i in range(0,5):\n",
    "        saver.restore(session, os.path.join(model_path,\"Model\"+str(i)))\n",
    "        print(\"iteration  \",(i+1))\n",
    "        eer_eval = get_EER(evaluation_path, (i+1), eval_sp_path)\n",
    "        print(\" evaluat eer  \",eer_eval)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"E:\\\\siva\\\\CQCC_CMVN_2017V2\\\\eval_context_data\"\n",
    "save_path = \"E:\\\\siva\\\\CQCC_CMVN_2017V2\\\\BN_DNN_bottle\\\\eval_BN_relu\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "session = tf.Session()\n",
    "saver.restore(session, model_path +str(31))\n",
    "save_bottleneck_features(train_path, save_path, \"train\", split = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottleneck_features(test_path, save_path, index, split = 1 ):\n",
    "    print(index)\n",
    "    \n",
    "    utterences = (len(list(os.listdir(test_path))))//2        \n",
    "    factor = int(utterences/split)\n",
    "    \n",
    "    for i in range(0, split):\n",
    "        \n",
    "        genuine_features = []\n",
    "        spoof_features = []\n",
    "        print(i*factor)\n",
    "        print((i+1)*factor)\n",
    "        for index in range(i*factor, (i+1)*factor):\n",
    "            if(index > utterences):\n",
    "                break\n",
    "            features = np.load(os.path.join(test_path, \"features\"+str(index+1)+\".npy\"))\n",
    "            labels = np.load(os.path.join(test_path, \"labels\"+str(index+1)+\".npy\"))\n",
    "\n",
    "            bn_features = session.run(bottleneck_out, feed_dict = {input_x : features})\n",
    "\n",
    "            if(labels[0][0] == 1):\n",
    "                genuine_features.append(bn_features)\n",
    "            else:\n",
    "                spoof_features.append(bn_features)\n",
    "\n",
    "        genuine_features = np.array(genuine_features)\n",
    "        spoof_features = np.array(spoof_features)\n",
    "    \n",
    "        print(genuine_features.shape)\n",
    "    #     print(genuine_features[0].shape)\n",
    "    #     print(genuine_features)\n",
    "        print(spoof_features.shape)\n",
    "\n",
    "        savemat(os.path.join(save_path, str(i+1)+\"genuine_\"+str(index) ), {'genuine':genuine_features})\n",
    "        savemat(os.path.join(save_path, str(i+1)+\"spoof_\"+str(index) ), {'spoof':spoof_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    for i in range(18,30):\n",
    "        saver.restore(session, model_path +str(i))\n",
    "        print(\"iteration  \",(i+1))\n",
    "        eer_eval = get_EER(evaluation_path, (i+1), eva_sp_path)\n",
    "        print(\" evaluat eer  \",eer_eval)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
